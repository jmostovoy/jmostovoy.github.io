<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>STA414 by duvenaud</title>

    <link rel="stylesheet" href="/sta414/assets/css/style.css?v=f952298195b69081ecfd30d8d9a414f04be27f77">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    
  <div class="wrapper">


      <h1 id="jonathan-mostovoys-github-homepage">Jonathan Mostovoy's GitHub Homepage</h1>
<h2 id="statistical-methods-for-machine-learning-and-data-mining">Statistical Methods for Machine Learning and Data Mining</h2>

<p><img src="https://raw.githubusercontent.com/jamesrobertlloyd/gpss-research/master/logo.png" width="500" /></p>

<p>This course introduces machine learning to students with a statistical background.  Besides teaching standard methods such as logistic and ridge regression, kernel density estimation, and random forests, this course course will try to offer a broader view of model-building and optimization using probabilistic building blocks.</p>

<h3 id="what-you-will-learn">What you will learn:</h3>

<ul>
  <li>Standard statistical learning algorithms, when to use them, and their limitations.</li>
  <li>The main elements of probabilistic models (distributions, expectations, latent variables, neural networks) and how to combine them.</li>
  <li>Standard computational tools (Monte Carlo, Stochastic optimization, regularization, automatic differentiation).</li>
</ul>

<h3 id="instructors">Instructors:</h3>

<ul>
  <li><a href="http://www.cs.toronto.edu/~duvenaud">David Duvenaud</a>, Office: 384 Pratt
    <ul>
      <li>Email: <a href="mailto:duvenaud@cs.toronto.edu">duvenaud@cs.toronto.edu</a> (put “STA414” in the subject)</li>
      <li>Lectures: Mondays 2-5pm, EM 119</li>
      <li>Office hours: Mondays 11-12 noon in Pratt Building, Room 384</li>
    </ul>
  </li>
  <li><a href="http://www.mebden.com/">Mark Ebden</a>, Office: SS6026C and PT371
    <ul>
      <li>Email: mark [dot] ebden [at] utoronto [dot] ca</li>
      <li>Lectures: Tuesdays 7-10 pm, SS1071</li>
      <li>Office Hours:  Thursdays 3-4 pm in SS6026C, and after each lecture just outside the classroom itself</li>
    </ul>
  </li>
  <li>The two instructors won’t stick strictly to lecturing in their own sections. For example, on 16/17 January David Duvenaud will teach both sections (0101 and 0501), and in future sometimes Mark Ebden will teach both. This will occur regularly.</li>
</ul>

<h3 id="teaching-assistants">Teaching Assistants:</h3>

<ul>
  <li>Amanjit Kainth</li>
  <li>Chris Cremer</li>
  <li>Luhui Gan</li>
  <li>Yang Guan Jian Guo (Tommy)</li>
</ul>

<p><a href="/sta414/syllabus.pdf">Syllabus and Course Information</a></p>

<p><a href="https://piazza.com/class/ivcpw2h2fq775m">Piazza</a></p>

<h2 id="tentative-schedule">Tentative Schedule</h2>

<ul>
  <li><strong>January 9 and 10:</strong> Introduction.
    <ul>
      <li><a href="/sta414/lectures/lec1.pdf">Intro + Background slides</a></li>
      <li><a href="/sta414/lectures/lec1-part2-edited.pdf">Basic supervised learning and probability slides</a></li>
      <li><a href="/sta414/lectures/skill-quiz.pdf">Background quiz</a></li>
    </ul>

    <p>Readings: <a href="http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/22.40.pdf">Chapter 2 of David Mackay’s textbook</a></p>
  </li>
  <li><strong>January 16 and 17:</strong>
    <ul>
      <li><a href="/sta414/lectures/Lecture2.pdf">The Exponential Family and beyond; Maximum Likelihood</a></li>
      <li><a href="/sta414/lectures/optimization.pdf">Optimization</a></li>
    </ul>

    <p>Readings:</p>

    <ul>
      <li><a href="http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/47.59.pdf">Chapter 3 of David Mackay’s textbook</a></li>
      <li><a href="http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html">Animations of different optimization algorithms</a></li>
    </ul>

    <p>Example code:</p>

    <ul>
      <li><a href="https://github.com/HIPS/autograd/blob/master/examples/logistic_regression.py">Logistic regression autograd example</a></li>
      <li><a href="https://github.com/HIPS/autograd/blob/master/examples/neural_net_regression.py">Neural net regression example</a></li>
      <li><a href="https://github.com/HIPS/autograd/blob/master/examples/gmm.py">Mixture of Gaussians example</a></li>
    </ul>
  </li>
  <li>
    <p><strong>January 23 and 24:</strong> <a href="/sta414/lectures/Lecture3.pdf">Linear basis function models, decision theory, classification</a></p>
  </li>
  <li>
    <p><strong>January 30 and 31:</strong> <a href="/sta414/lectures/Lecture4.pdf">Bayesian inference, and kNN</a></p>
  </li>
  <li>
    <p><strong>February 5:</strong> <a href="/sta414/assignments/HW1.pdf">Assignment 1</a> due.</p>
  </li>
  <li><strong>February 6 and 7:</strong> <a href="/sta414/lectures/Lecture5.pdf">Classification</a>
    <ul>
      <li><a href="http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.15656&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false">Classifier neural network demo</a></li>
    </ul>
  </li>
  <li>
    <p><strong>February 13:</strong> Midterm exam for both sections. (No class on Feb 14.) <a href="/sta414/midtermMarksSTA414.png">Grade distribution</a></p>
  </li>
  <li>
    <p><strong>February 17 to 26:</strong> Reading week</p>
  </li>
  <li>
    <p><strong>February 27 and 28:</strong> <a href="/sta414/lectures/Lecture6-mixtures.pdf">Mixture models</a></p>
  </li>
  <li>
    <p><strong>March 6 and 7:</strong> <a href="/sta414/lectures/Lecture7.pdf">Continuous Latent variable models, and neural networks</a> Note: PPCA is now bonus material.</p>
  </li>
  <li>
    <p><strong>March 12:</strong>  <a href="/sta414/assignments/assignment2.pdf">Assignment 2</a> due.</p>

    <ul>
      <li>Helper code in <a href="/sta414/assignments/loadMNIST.R">R</a></li>
      <li>Helper code in <a href="/sta414/assignments/data.py">Python</a></li>
      <li><a href="https://github.com/HIPS/autograd/blob/master/examples/logistic_regression.py">Binary Logistic regression example</a></li>
      <li>Some Python and Numpy resources, copied from <a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/">Roger Grosse’s neural networks course</a>:
        <ul>
          <li><a href="https://store.continuum.io/cshop/anaconda/">Anaconda</a> provides an installer for Python and Numpy for Windows, Linux, and Mac.</li>
          <li><a href="http://www.engr.ucsb.edu/~shell/che210d/numpy.pdf">Numpy tutorial</a></li>
          <li><a href="http://learnxinyminutes.com/docs/python/">Learn X in Y minutes</a> can get you up to speed in Python if you already know other languages.</li>
          <li><a href="https://courses.edx.org/courses/MITx/6.00.1_4x/3T2014/courseware/Week_0/">Lectures 2, 3, 4, and 6 for MIT’s intro programming course</a> Can help you get started if you don’t have much programming background.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>March 13 and 14:</strong> <a href="/sta414/lectures/lecture8-sampling.pdf">Sampling and Monte Carlo methods</a></p>

    <ul>
      <li>Readings: <a href="http://www.inference.phy.cam.ac.uk/itprnn/book.pdf">Chapter 29 of David Mackay’s textbook</a></li>
      <li>Demos: <a href="https://chi-feng.github.io/mcmc-demo/">Interactive MCMC demos</a></li>
    </ul>
  </li>
  <li>
    <p><strong>March 20 and 21:</strong> <a href="/sta414/lectures/Lecture9_2017.pdf">Graphical models, and modelling sequential data</a></p>
  </li>
  <li>
    <p><strong>March 27 and 28:</strong> <a href="/sta414/lectures/09-svi.pdf">Stochastic Variational Inference and Variational autoencoders</a></p>

    <ul>
      <li>Optional Reading: <a href="https://arxiv.org/pdf/1601.00670.pdf">Variational Inference: A Review for Statisticians</a></li>
    </ul>
  </li>
  <li>
    <p><strong>April 1:</strong> Assignment 3 due at 1 pm. Questions are <a href="/sta414/assignments/assignment3.pdf">here</a></p>
  </li>
  <li>
    <p><strong>April 3 and 4:</strong> <a href="/sta414/lectures/Lecture11_GPs_2.pdf">Gaussian processes</a></p>
  </li>
  <li><strong>April 21:</strong> Exam. Some warm-up problems are <a href="/sta414/assignments/practiceQs.pdf">here</a>, and the first page of the exam will be posted soon.</li>
</ul>




    <script src="/sta414/assets/js/scale.fix.js"></script>
  </div>

  
    <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
      document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
      try {
        var pageTracker = _gat._getTracker("53180609");
      pageTracker._trackPageview();
      } catch(err) {}
    </script>
  
  </body>
</html>
